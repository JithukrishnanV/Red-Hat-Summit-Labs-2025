# 🚀 OpenShift AI Labs: Deploying Private LLMs

This repository provides **hands-on labs** for learning how to securely deploy, manage, and consume **Large Language Models (LLMs)** on **Red Hat OpenShift AI**.  
The labs are designed to help different roles in your organization — from platform engineers to developers and decision makers — adopt AI at scale in a structured and cost-efficient way.

---

## 📌 What You’ll Learn

### 🔧 Platform Engineers
- Build a **Model-as-a-Service (MaaS)** layer for your organization  
- Manage models with **versioning, rollbacks, and lifecycle policies**  
- Provide **secure API endpoints** with authentication, rate limiting, and observability  
- Pool GPU resources to reduce waste and enforce fair usage  

### 👩‍💻 AI Developers
- Connect IDE extensions (e.g., **Continue**) to private LLM endpoints  
- Use organization-approved models for **coding assistance, prototyping, and apps**  
- Focus on building AI-powered features without worrying about GPU infrastructure  

### ⚙️ DevOps Engineers
- Leverage **agentic AI and MCP servers** to automate cluster tasks  
- Enable LLMs to interact directly with OpenShift and external services (e.g., **Slack alerts**)  
- Enhance observability and response automation with AI-driven workflows  

### 📊 Technical Decision Makers
- Track model usage and adoption with **API analytics dashboards**  
- Optimize infrastructure spend with pooled GPU resources and **smaller, domain-specific models**  
- Confidently evaluate **cost, performance, and business impact** of LLM deployments  

---

## 🧪 Lab Structure

Each lab is role-specific but connected:

1. **Lab 1: Model-as-a-Service Setup**  
   Deploy a model to OpenShift AI, configure lifecycle management, and expose it securely.  

2. **Lab 2: Developer Productivity with LLMs**  
   Connect developer tools to the model endpoint and integrate LLMs into workflows.  

3. **Lab 3: Agentic AI for Operations**  
   Use MCP servers to extend LLM capabilities into infrastructure automation.  

4. **Lab 4: API Analytics & Optimization**  
   Visualize API usage, enforce policies, and deploy smaller specialized models in production.  

---

## 🏁 Outcomes

By completing these labs, you will:  
- Understand how to scale LLMs securely in enterprise environments  
- Provide **developer-friendly APIs** without exposing raw infrastructure  
- Control costs while increasing model adoption across teams  
- Standardize AI governance, monitoring, and security practices  

---

💡 These labs are a **practical guide for enterprises** looking to move from ad-hoc GPU usage to a **centralized, secure, and efficient Model-as-a-Service platform** on OpenShift AI.  

---

## 📂 Coming Soon
- Example configs and YAML templates  
- Reference architecture diagrams  
- Scripts for setting up GPU pools and observability
